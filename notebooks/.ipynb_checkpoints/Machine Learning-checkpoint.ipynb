{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What will happen to a dog in the shelter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df = pd.read_csv(\"../data/unique_austin_shelter.csv\")\n",
    "unique_df.set_index(\"animal_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['is_mixed','intake_condition', 'intake_type', 'sex', 'fixed', 'time_in_shelter', 'age_in']\n",
    "features1 = ['intake_condition', 'intake_type', 'sex', 'fixed', 'time_in_shelter', 'age_in']\n",
    "features2 = ['is_mixed', 'sex', 'fixed', 'time_in_shelter', 'age_in']\n",
    "features3 = ['intake_condition', 'sex', 'time_in_shelter', 'age_in']\n",
    "features4 = ['is_mixed', 'intake_type', 'sex', 'fixed', 'age_in']\n",
    "features5 = ['is_mixed', 'sex', 'fixed', 'age_in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unique_df = unique_df[unique_df.in_shelter == \"No\"][['is_mixed', 'intake_condition', 'intake_type', 'name', 'sex', 'fixed', 'time_in_shelter', 'has_name', 'age_in','outcome_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = new_unique_df.time_in_shelter.apply(pd.to_timedelta)\n",
    "temp = temp.apply(lambda x:( x.days*24*60*60 + x.seconds)/60)\n",
    "new_unique_df.time_in_shelter = temp\n",
    "new_unique_df.time_in_shelter.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unique_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_mixed</th>\n",
       "      <th>intake_condition</th>\n",
       "      <th>intake_type</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>fixed</th>\n",
       "      <th>time_in_shelter</th>\n",
       "      <th>has_name</th>\n",
       "      <th>age_in</th>\n",
       "      <th>outcome_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animal_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A786884</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Brock</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7132.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>Transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A706918</th>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Belle</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.005479</td>\n",
       "      <td>Return to Owner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A724273</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Runster</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>9994.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994521</td>\n",
       "      <td>Return to Owner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A778404</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Owner Surrender</td>\n",
       "      <td>Max</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>4784.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.002740</td>\n",
       "      <td>Adoption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A682524</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Rio</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4538.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.002740</td>\n",
       "      <td>Return to Owner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           is_mixed intake_condition      intake_type     name     sex fixed  \\\n",
       "animal_id                                                                      \n",
       "A786884           1           Normal            Stray    Brock    Male   Yes   \n",
       "A706918           0           Normal            Stray    Belle  Female   Yes   \n",
       "A724273           1           Normal            Stray  Runster    Male    No   \n",
       "A778404           1           Normal  Owner Surrender      Max    Male    No   \n",
       "A682524           1           Normal            Stray      Rio    Male   Yes   \n",
       "\n",
       "           time_in_shelter  has_name    age_in     outcome_type  \n",
       "animal_id                                                        \n",
       "A786884             7132.0         1  2.000000         Transfer  \n",
       "A706918              134.0         1  8.005479  Return to Owner  \n",
       "A724273             9994.0         1  0.994521  Return to Owner  \n",
       "A778404             4784.0         1  4.002740         Adoption  \n",
       "A682524             4538.0         1  4.002740  Return to Owner  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_unique_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train_dict = new_unique_df[features].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(x_train_dict)\n",
    "x_train = vec.transform(x_train_dict)\n",
    "\n",
    "scaler = Normalizer()\n",
    "scaler.fit(x_train)\n",
    "x_train_sc = scaler.transform(x_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = KNeighborsClassifier(n_neighbors=9)\n",
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vec), \n",
    "    (\"scaler\", scaler), \n",
    "    (\"model\", model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: KNeighborsClassifier \n",
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is_mixed', 'intake_condition', 'intake_type', 'sex', 'fixed', 'time_in_shelter', 'age_in'] 0.6971530504115181\n",
      "['intake_condition', 'intake_type', 'sex', 'fixed', 'time_in_shelter', 'age_in'] 0.70163558915865\n",
      "['is_mixed', 'sex', 'fixed', 'time_in_shelter', 'age_in'] 0.6404590170368781\n",
      "['intake_condition', 'sex', 'time_in_shelter', 'age_in'] 0.647650275690558\n",
      "['is_mixed', 'intake_type', 'sex', 'fixed', 'age_in'] 0.5567718064738866\n",
      "['is_mixed', 'sex', 'fixed', 'age_in'] 0.5119780145296502\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=30, n_jobs=-1)\n",
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vec), \n",
    "    (\"scaler\", scaler), \n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "\n",
    "for feat in [features, features1, features2, features3, features4, features5]:\n",
    "    x_train_dict = new_unique_df[feat].to_dict(orient=\"records\")\n",
    "    y_train = new_unique_df['outcome_type']\n",
    "    print(feat, cross_val_score(pipeline, x_train_dict, y_train, cv=5, scoring=\"accuracy\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict = new_unique_df[features1].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), Normalizer(), MinMaxScaler(), MaxAbsScaler(), RobustScaler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   39.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:  1.7min remaining:   58.8s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:  2.0min remaining:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'scaler': [StandardScaler(copy=True, with_mean=True, with_std=True), Normalizer(copy=True, norm='l2'), MinMaxScaler(copy=True, feature_range=(0, 1)), MaxAbsScaler(copy=True), RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_kn = GridSearchCV(pipeline,param_grid=dict(scaler=scalers),cv=5, verbose=5, n_jobs=-1)\n",
    "grid_kn.fit(x_train_dict, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7065734832371113\n",
      "[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=False)), ('scaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "       with_scaling=True)), ('model', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
      "           weights='uniform'))]\n"
     ]
    }
   ],
   "source": [
    "print(grid_kn.best_score_)\n",
    "print(grid_kn.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_scaler</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.052805</td>\n",
       "      <td>2.826535</td>\n",
       "      <td>7.965209</td>\n",
       "      <td>3.085577</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>{'scaler': StandardScaler(copy=True, with_mean...</td>\n",
       "      <td>0.699190</td>\n",
       "      <td>0.706582</td>\n",
       "      <td>0.702925</td>\n",
       "      <td>0.702545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701636</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>2</td>\n",
       "      <td>0.715290</td>\n",
       "      <td>0.713788</td>\n",
       "      <td>0.716094</td>\n",
       "      <td>0.715525</td>\n",
       "      <td>0.717116</td>\n",
       "      <td>0.715563</td>\n",
       "      <td>0.001088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.966998</td>\n",
       "      <td>0.564824</td>\n",
       "      <td>5.674741</td>\n",
       "      <td>0.882539</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>{'scaler': Normalizer(copy=True, norm='l2')}</td>\n",
       "      <td>0.698810</td>\n",
       "      <td>0.707215</td>\n",
       "      <td>0.699380</td>\n",
       "      <td>0.699253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700344</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>3</td>\n",
       "      <td>0.713928</td>\n",
       "      <td>0.711731</td>\n",
       "      <td>0.715145</td>\n",
       "      <td>0.714480</td>\n",
       "      <td>0.716388</td>\n",
       "      <td>0.714334</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.812916</td>\n",
       "      <td>1.190793</td>\n",
       "      <td>4.673145</td>\n",
       "      <td>1.449552</td>\n",
       "      <td>MinMaxScaler(copy=True, feature_range=(0, 1))</td>\n",
       "      <td>{'scaler': MinMaxScaler(copy=True, feature_ran...</td>\n",
       "      <td>0.696786</td>\n",
       "      <td>0.699873</td>\n",
       "      <td>0.695327</td>\n",
       "      <td>0.698493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696647</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>4</td>\n",
       "      <td>0.711301</td>\n",
       "      <td>0.709673</td>\n",
       "      <td>0.711568</td>\n",
       "      <td>0.711537</td>\n",
       "      <td>0.712654</td>\n",
       "      <td>0.711347</td>\n",
       "      <td>0.000959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.145111</td>\n",
       "      <td>0.846617</td>\n",
       "      <td>7.430025</td>\n",
       "      <td>0.390906</td>\n",
       "      <td>MaxAbsScaler(copy=True)</td>\n",
       "      <td>{'scaler': MaxAbsScaler(copy=True)}</td>\n",
       "      <td>0.696786</td>\n",
       "      <td>0.699747</td>\n",
       "      <td>0.694821</td>\n",
       "      <td>0.698620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696495</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>5</td>\n",
       "      <td>0.711206</td>\n",
       "      <td>0.709452</td>\n",
       "      <td>0.711505</td>\n",
       "      <td>0.711537</td>\n",
       "      <td>0.712590</td>\n",
       "      <td>0.711258</td>\n",
       "      <td>0.001018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.040941</td>\n",
       "      <td>0.654370</td>\n",
       "      <td>3.469866</td>\n",
       "      <td>1.585345</td>\n",
       "      <td>RobustScaler(copy=True, quantile_range=(25.0, ...</td>\n",
       "      <td>{'scaler': RobustScaler(copy=True, quantile_ra...</td>\n",
       "      <td>0.703493</td>\n",
       "      <td>0.711013</td>\n",
       "      <td>0.706597</td>\n",
       "      <td>0.706091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706573</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718740</td>\n",
       "      <td>0.717840</td>\n",
       "      <td>0.719639</td>\n",
       "      <td>0.721190</td>\n",
       "      <td>0.721389</td>\n",
       "      <td>0.719760</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       5.052805      2.826535         7.965209        3.085577   \n",
       "1       2.966998      0.564824         5.674741        0.882539   \n",
       "2       3.812916      1.190793         4.673145        1.449552   \n",
       "3       5.145111      0.846617         7.430025        0.390906   \n",
       "4       2.040941      0.654370         3.469866        1.585345   \n",
       "\n",
       "                                        param_scaler  \\\n",
       "0  StandardScaler(copy=True, with_mean=True, with...   \n",
       "1                   Normalizer(copy=True, norm='l2')   \n",
       "2      MinMaxScaler(copy=True, feature_range=(0, 1))   \n",
       "3                            MaxAbsScaler(copy=True)   \n",
       "4  RobustScaler(copy=True, quantile_range=(25.0, ...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'scaler': StandardScaler(copy=True, with_mean...           0.699190   \n",
       "1       {'scaler': Normalizer(copy=True, norm='l2')}           0.698810   \n",
       "2  {'scaler': MinMaxScaler(copy=True, feature_ran...           0.696786   \n",
       "3                {'scaler': MaxAbsScaler(copy=True)}           0.696786   \n",
       "4  {'scaler': RobustScaler(copy=True, quantile_ra...           0.703493   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           0.706582           0.702925           0.702545  ...   \n",
       "1           0.707215           0.699380           0.699253  ...   \n",
       "2           0.699873           0.695327           0.698493  ...   \n",
       "3           0.699747           0.694821           0.698620  ...   \n",
       "4           0.711013           0.706597           0.706091  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.701636        0.003318                2            0.715290   \n",
       "1         0.700344        0.003535                3            0.713928   \n",
       "2         0.696647        0.002479                4            0.711301   \n",
       "3         0.696495        0.002604                5            0.711206   \n",
       "4         0.706573        0.002460                1            0.718740   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            0.713788            0.716094            0.715525   \n",
       "1            0.711731            0.715145            0.714480   \n",
       "2            0.709673            0.711568            0.711537   \n",
       "3            0.709452            0.711505            0.711537   \n",
       "4            0.717840            0.719639            0.721190   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.717116          0.715563         0.001088  \n",
       "1            0.716388          0.714334         0.001539  \n",
       "2            0.712654          0.711347         0.000959  \n",
       "3            0.712590          0.711258         0.001018  \n",
       "4            0.721389          0.719760         0.001374  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_kn.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust scaler is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K value testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict = new_unique_df[features1].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(x_train_dict)\n",
    "x_train = vec.transform(x_train_dict)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_sc = scaler.transform(x_train)\n",
    "\n",
    "model = KNeighborsClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [5,10,20,30,40,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': [5, 10, 20, 30, 40, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_kn_k = GridSearchCV(model,param_grid=dict(n_neighbors=ks),cv=5)\n",
    "grid_kn_k.fit(x_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071558796718322\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(grid_kn_k.best_score_)\n",
    "print(grid_kn_k.best_estimator_.n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_kn_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-e88bd6737f1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_kn_k\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"param_n_neighbors\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_kn_k' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(grid_kn_k.cv_results_).set_index(\"param_n_neighbors\")['mean_test_score'].plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = 0.7071558796718322\n",
    "\n",
    "k = 40\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "features = features1\n",
    "\n",
    "model = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
    "           weights='uniform'),\n",
    "       fit_params=None, iid='warn', n_jobs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is_mixed', 'intake_condition', 'intake_type', 'sex', 'fixed', 'time_in_shelter', 'age_in'] 0.6530440308125804\n",
      "['intake_condition', 'intake_type', 'sex', 'fixed', 'time_in_shelter', 'age_in'] 0.6509176501850181\n",
      "['is_mixed', 'sex', 'fixed', 'time_in_shelter', 'age_in'] 0.5705971276166315\n",
      "['intake_condition', 'sex', 'time_in_shelter', 'age_in'] 0.5759396983858641\n",
      "['is_mixed', 'intake_type', 'sex', 'fixed', 'age_in'] 0.5650260612953572\n",
      "['is_mixed', 'sex', 'fixed', 'age_in'] 0.5228661492148154\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vec), \n",
    "    (\"scaler\", scaler), \n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "\n",
    "for feat in [features, features1, features2, features3, features4, features5]:\n",
    "    x_train_dict = new_unique_df[feat].to_dict(orient=\"records\")\n",
    "    y_train = new_unique_df['outcome_type']\n",
    "    print(feat, cross_val_score(pipeline, x_train_dict, y_train, cv=5, scoring=\"accuracy\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict = new_unique_df[features].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), Normalizer(), MinMaxScaler(), MaxAbsScaler(), RobustScaler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:   16.6s remaining:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   22.3s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   25.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None..._jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'scaler': [StandardScaler(copy=True, with_mean=True, with_std=True), Normalizer(copy=True, norm='l2'), MinMaxScaler(copy=True, feature_range=(0, 1)), MaxAbsScaler(copy=True), RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=5)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf = GridSearchCV(pipeline,param_grid=dict(scaler=scalers),cv=5, verbose=5, n_jobs=-1)\n",
    "grid_rf.fit(x_train_dict, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7210574293527803\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=90, max_features=2, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=15,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
      "            oob_score=False, random_state=10, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid_rf.best_score_)\n",
    "print(grid_rf.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_scaler</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.463739</td>\n",
       "      <td>0.049157</td>\n",
       "      <td>0.153403</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>{'scaler': StandardScaler(copy=True, with_mean...</td>\n",
       "      <td>0.644394</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>0.659111</td>\n",
       "      <td>0.648854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652664</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>4</td>\n",
       "      <td>0.952137</td>\n",
       "      <td>0.952994</td>\n",
       "      <td>0.952018</td>\n",
       "      <td>0.950055</td>\n",
       "      <td>0.951766</td>\n",
       "      <td>0.951794</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.461668</td>\n",
       "      <td>0.017738</td>\n",
       "      <td>0.153075</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>{'scaler': Normalizer(copy=True, norm='l2')}</td>\n",
       "      <td>0.678562</td>\n",
       "      <td>0.690380</td>\n",
       "      <td>0.686590</td>\n",
       "      <td>0.683804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684468</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907249</td>\n",
       "      <td>0.906274</td>\n",
       "      <td>0.907169</td>\n",
       "      <td>0.904763</td>\n",
       "      <td>0.906254</td>\n",
       "      <td>0.906342</td>\n",
       "      <td>0.000896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.444430</td>\n",
       "      <td>0.044151</td>\n",
       "      <td>0.157617</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>MinMaxScaler(copy=True, feature_range=(0, 1))</td>\n",
       "      <td>{'scaler': MinMaxScaler(copy=True, feature_ran...</td>\n",
       "      <td>0.646925</td>\n",
       "      <td>0.659873</td>\n",
       "      <td>0.659238</td>\n",
       "      <td>0.651387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653474</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>3</td>\n",
       "      <td>0.952168</td>\n",
       "      <td>0.950842</td>\n",
       "      <td>0.952366</td>\n",
       "      <td>0.950404</td>\n",
       "      <td>0.951450</td>\n",
       "      <td>0.951446</td>\n",
       "      <td>0.000751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.438518</td>\n",
       "      <td>0.040227</td>\n",
       "      <td>0.150395</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>MaxAbsScaler(copy=True)</td>\n",
       "      <td>{'scaler': MaxAbsScaler(copy=True)}</td>\n",
       "      <td>0.643888</td>\n",
       "      <td>0.660253</td>\n",
       "      <td>0.659491</td>\n",
       "      <td>0.649994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653575</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>2</td>\n",
       "      <td>0.953973</td>\n",
       "      <td>0.951602</td>\n",
       "      <td>0.953315</td>\n",
       "      <td>0.951828</td>\n",
       "      <td>0.951734</td>\n",
       "      <td>0.952490</td>\n",
       "      <td>0.000967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.391922</td>\n",
       "      <td>0.052007</td>\n",
       "      <td>0.153726</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>RobustScaler(copy=True, quantile_range=(25.0, ...</td>\n",
       "      <td>{'scaler': RobustScaler(copy=True, quantile_ra...</td>\n",
       "      <td>0.639205</td>\n",
       "      <td>0.659241</td>\n",
       "      <td>0.653413</td>\n",
       "      <td>0.650627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651296</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>5</td>\n",
       "      <td>0.952422</td>\n",
       "      <td>0.950082</td>\n",
       "      <td>0.952113</td>\n",
       "      <td>0.952682</td>\n",
       "      <td>0.950817</td>\n",
       "      <td>0.951623</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.463739      0.049157         0.153403        0.002868   \n",
       "1       0.461668      0.017738         0.153075        0.003597   \n",
       "2       0.444430      0.044151         0.157617        0.008575   \n",
       "3       0.438518      0.040227         0.150395        0.002422   \n",
       "4       0.391922      0.052007         0.153726        0.004880   \n",
       "\n",
       "                                        param_scaler  \\\n",
       "0  StandardScaler(copy=True, with_mean=True, with...   \n",
       "1                   Normalizer(copy=True, norm='l2')   \n",
       "2      MinMaxScaler(copy=True, feature_range=(0, 1))   \n",
       "3                            MaxAbsScaler(copy=True)   \n",
       "4  RobustScaler(copy=True, quantile_range=(25.0, ...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'scaler': StandardScaler(copy=True, with_mean...           0.644394   \n",
       "1       {'scaler': Normalizer(copy=True, norm='l2')}           0.678562   \n",
       "2  {'scaler': MinMaxScaler(copy=True, feature_ran...           0.646925   \n",
       "3                {'scaler': MaxAbsScaler(copy=True)}           0.643888   \n",
       "4  {'scaler': RobustScaler(copy=True, quantile_ra...           0.639205   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           0.658228           0.659111           0.648854  ...   \n",
       "1           0.690380           0.686590           0.683804  ...   \n",
       "2           0.659873           0.659238           0.651387  ...   \n",
       "3           0.660253           0.659491           0.649994  ...   \n",
       "4           0.659241           0.653413           0.650627  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.652664        0.005576                4            0.952137   \n",
       "1         0.684468        0.003923                1            0.907249   \n",
       "2         0.653474        0.005174                3            0.952168   \n",
       "3         0.653575        0.006112                2            0.953973   \n",
       "4         0.651296        0.006658                5            0.952422   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            0.952994            0.952018            0.950055   \n",
       "1            0.906274            0.907169            0.904763   \n",
       "2            0.950842            0.952366            0.950404   \n",
       "3            0.951602            0.953315            0.951828   \n",
       "4            0.950082            0.952113            0.952682   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.951766          0.951794         0.000963  \n",
       "1            0.906254          0.906342         0.000896  \n",
       "2            0.951450          0.951446         0.000751  \n",
       "3            0.951734          0.952490         0.000967  \n",
       "4            0.950817          0.951623         0.001002  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_rf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict = new_unique_df[features].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(x_train_dict)\n",
    "x_train = vec.transform(x_train_dict)\n",
    "\n",
    "scaler = Normalizer()\n",
    "scaler.fit(x_train)\n",
    "x_train_sc = scaler.transform(x_train)\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [10, 50, 90, 100],\n",
    "    'max_features': [2, 3],\n",
    "    'random_state': [10],\n",
    "    'min_samples_leaf': [2, 3, 4],\n",
    "    'min_samples_split': [10, 12, 15],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   54.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 37.5min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 64.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed: 97.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'bootstrap': [True], 'max_depth': [10, 50, 90, 100], 'max_features': [2, 3], 'random_state': [10], 'min_samples_leaf': [2, 3, 4], 'min_samples_split': [10, 12, 15], 'n_estimators': [100, 200, 300, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_p = GridSearchCV(model,param_grid=param_grid,cv=5, n_jobs=-1, verbose=2)\n",
    "grid_rf_p.fit(x_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7210574293527803\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=90, max_features=2, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=15,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
      "            oob_score=False, random_state=10, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid_rf_p.best_score_)\n",
    "print(grid_rf_p.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = 0.7210574293527803\n",
    "\n",
    "scaler = Normalizer()\n",
    "\n",
    "features = features\n",
    "\n",
    "model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=90, max_features=2, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=2, min_samples_split=15,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
    "            oob_score=False, random_state=10, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(verbose=2)\n",
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vec), \n",
    "    (\"scaler\", scaler), \n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "\n",
    "for feat in [features, features1, features2, features3, features4, features5]:\n",
    "    x_train_dict = new_unique_df[feat].to_dict(orient=\"records\")\n",
    "    y_train = new_unique_df['outcome_type']\n",
    "    print(feat, cross_val_score(pipeline, x_train_dict, y_train, cv=5, scoring=\"accuracy\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict = new_unique_df[features].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), Normalizer(), MinMaxScaler(), MaxAbsScaler(), RobustScaler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:  1.7min remaining:   59.0s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:  2.0min remaining:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=2))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'scaler': [StandardScaler(copy=True, with_mean=True, with_std=True), Normalizer(copy=True, norm='l2'), MinMaxScaler(copy=True, feature_range=(0, 1)), MaxAbsScaler(copy=True), RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=5)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ls = GridSearchCV(pipeline,param_grid=dict(scaler=scalers),cv=5, verbose=5, n_jobs=-1)\n",
    "grid_ls.fit(x_train_dict, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6214423174313785\n",
      "[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=False)), ('scaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "       with_scaling=True)), ('model', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=2))]\n"
     ]
    }
   ],
   "source": [
    "print(grid_ls.best_score_)\n",
    "print(grid_ls.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict = new_unique_df[features].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(x_train_dict)\n",
    "x_train = vec.transform(x_train_dict)\n",
    "\n",
    "scaler = Normalizer()\n",
    "scaler.fit(x_train)\n",
    "x_train_sc = scaler.transform(x_train)\n",
    "\n",
    "model = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'max_iter': [500,1000,1500]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   54.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 37.5min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 64.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed: 97.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'bootstrap': [True], 'max_depth': [10, 50, 90, 100], 'max_features': [2, 3], 'random_state': [10], 'min_samples_leaf': [2, 3, 4], 'min_samples_split': [10, 12, 15], 'n_estimators': [100, 200, 300, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ls_p = GridSearchCV(model,param_grid=param_grid,cv=5, n_jobs=-1, verbose=2)\n",
    "grid_ls_p.fit(x_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7210574293527803\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=90, max_features=2, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=15,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
      "            oob_score=False, random_state=10, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid_ls_p.best_score_)\n",
    "print(grid_ls_p.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = \n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "features = features\n",
    "\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingClassifier(n_jobs=-1)\n",
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vec), \n",
    "    (\"scaler\", scaler), \n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "\n",
    "for feat in [features, features1, features2, features3, features4, features5]:\n",
    "    x_train_dict = new_unique_df[feat].to_dict(orient=\"records\")\n",
    "    y_train = new_unique_df['outcome_type']\n",
    "    print(feat, cross_val_score(pipeline, x_train_dict, y_train, cv=5, scoring=\"accuracy\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict = new_unique_df[features].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), Normalizer(), MinMaxScaler(), MaxAbsScaler(), RobustScaler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:   23.0s remaining:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   30.7s remaining:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   34.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "         n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
       "         verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'scaler': [StandardScaler(copy=True, with_mean=True, with_std=True), Normalizer(copy=True, norm='l2'), MinMaxScaler(copy=True, feature_range=(0, 1)), MaxAbsScaler(copy=True), RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=5)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_bc = GridSearchCV(pipeline,param_grid=dict(scaler=scalers),cv=5, verbose=5, n_jobs=-1)\n",
    "grid_bc.fit(x_train_dict, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6745670009115771\n",
      "[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=False)), ('scaler', Normalizer(copy=True, norm='l2')), ('model', BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False))]\n"
     ]
    }
   ],
   "source": [
    "print(grid_bc.best_score_)\n",
    "print(grid_bc.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_scaler</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.554864</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.152806</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>{'scaler': StandardScaler(copy=True, with_mean...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.680506</td>\n",
       "      <td>0.675066</td>\n",
       "      <td>0.672914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673554</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>2</td>\n",
       "      <td>0.958341</td>\n",
       "      <td>0.958154</td>\n",
       "      <td>0.958569</td>\n",
       "      <td>0.959994</td>\n",
       "      <td>0.958096</td>\n",
       "      <td>0.958631</td>\n",
       "      <td>0.000701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.943844</td>\n",
       "      <td>0.059353</td>\n",
       "      <td>0.161845</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>{'scaler': Normalizer(copy=True, norm='l2')}</td>\n",
       "      <td>0.667426</td>\n",
       "      <td>0.678734</td>\n",
       "      <td>0.677219</td>\n",
       "      <td>0.677219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674567</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913454</td>\n",
       "      <td>0.913712</td>\n",
       "      <td>0.913531</td>\n",
       "      <td>0.914069</td>\n",
       "      <td>0.916065</td>\n",
       "      <td>0.914166</td>\n",
       "      <td>0.000973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.564024</td>\n",
       "      <td>0.022012</td>\n",
       "      <td>0.151201</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>MinMaxScaler(copy=True, feature_range=(0, 1))</td>\n",
       "      <td>{'scaler': MinMaxScaler(copy=True, feature_ran...</td>\n",
       "      <td>0.667046</td>\n",
       "      <td>0.679367</td>\n",
       "      <td>0.674940</td>\n",
       "      <td>0.674053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672567</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>4</td>\n",
       "      <td>0.960779</td>\n",
       "      <td>0.958692</td>\n",
       "      <td>0.959044</td>\n",
       "      <td>0.958253</td>\n",
       "      <td>0.958444</td>\n",
       "      <td>0.959042</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.550593</td>\n",
       "      <td>0.009570</td>\n",
       "      <td>0.150770</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>MaxAbsScaler(copy=True)</td>\n",
       "      <td>{'scaler': MaxAbsScaler(copy=True)}</td>\n",
       "      <td>0.667553</td>\n",
       "      <td>0.680127</td>\n",
       "      <td>0.676586</td>\n",
       "      <td>0.672281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672870</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>3</td>\n",
       "      <td>0.959481</td>\n",
       "      <td>0.958945</td>\n",
       "      <td>0.958506</td>\n",
       "      <td>0.958569</td>\n",
       "      <td>0.958286</td>\n",
       "      <td>0.958757</td>\n",
       "      <td>0.000419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.006365</td>\n",
       "      <td>0.151941</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>RobustScaler(copy=True, quantile_range=(25.0, ...</td>\n",
       "      <td>{'scaler': RobustScaler(copy=True, quantile_ra...</td>\n",
       "      <td>0.665781</td>\n",
       "      <td>0.676203</td>\n",
       "      <td>0.675446</td>\n",
       "      <td>0.668735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670389</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>5</td>\n",
       "      <td>0.960051</td>\n",
       "      <td>0.960401</td>\n",
       "      <td>0.956924</td>\n",
       "      <td>0.960215</td>\n",
       "      <td>0.960913</td>\n",
       "      <td>0.959701</td>\n",
       "      <td>0.001418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.554864      0.002476         0.152806        0.002147   \n",
       "1       0.943844      0.059353         0.161845        0.010232   \n",
       "2       0.564024      0.022012         0.151201        0.000739   \n",
       "3       0.550593      0.009570         0.150770        0.001580   \n",
       "4       0.562500      0.006365         0.151941        0.001624   \n",
       "\n",
       "                                        param_scaler  \\\n",
       "0  StandardScaler(copy=True, with_mean=True, with...   \n",
       "1                   Normalizer(copy=True, norm='l2')   \n",
       "2      MinMaxScaler(copy=True, feature_range=(0, 1))   \n",
       "3                            MaxAbsScaler(copy=True)   \n",
       "4  RobustScaler(copy=True, quantile_range=(25.0, ...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'scaler': StandardScaler(copy=True, with_mean...           0.666667   \n",
       "1       {'scaler': Normalizer(copy=True, norm='l2')}           0.667426   \n",
       "2  {'scaler': MinMaxScaler(copy=True, feature_ran...           0.667046   \n",
       "3                {'scaler': MaxAbsScaler(copy=True)}           0.667553   \n",
       "4  {'scaler': RobustScaler(copy=True, quantile_ra...           0.665781   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           0.680506           0.675066           0.672914  ...   \n",
       "1           0.678734           0.677219           0.677219  ...   \n",
       "2           0.679367           0.674940           0.674053  ...   \n",
       "3           0.680127           0.676586           0.672281  ...   \n",
       "4           0.676203           0.675446           0.668735  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.673554        0.004458                2            0.958341   \n",
       "1         0.674567        0.004192                1            0.913454   \n",
       "2         0.672567        0.004712                4            0.960779   \n",
       "3         0.672870        0.004914                3            0.959481   \n",
       "4         0.670389        0.004574                5            0.960051   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            0.958154            0.958569            0.959994   \n",
       "1            0.913712            0.913531            0.914069   \n",
       "2            0.958692            0.959044            0.958253   \n",
       "3            0.958945            0.958506            0.958569   \n",
       "4            0.960401            0.956924            0.960215   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.958096          0.958631         0.000701  \n",
       "1            0.916065          0.914166         0.000973  \n",
       "2            0.958444          0.959042         0.000908  \n",
       "3            0.958286          0.958757         0.000419  \n",
       "4            0.960913          0.959701         0.001418  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_bc.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict = new_unique_df[features].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(x_train_dict)\n",
    "x_train = vec.transform(x_train_dict)\n",
    "\n",
    "scaler = Normalizer()\n",
    "scaler.fit(x_train)\n",
    "x_train_sc = scaler.transform(x_train)\n",
    "\n",
    "model = BaggingClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_samples': [600,800,1000],\n",
    "    'max_features': [0.5, 0.75, 1],\n",
    "    'random_state': [10],\n",
    "    'n_estimators': [100, 200, 300, 500]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:   11.1s remaining:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   12.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "         n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
       "         verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_samples': [1600], 'max_features': [1], 'random_state': [10], 'n_estimators': [100, 200, 300, 500]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_bc_p = GridSearchCV(model,param_grid=param_grid,cv=5, n_jobs=-1, verbose=2)\n",
    "grid_bc_p.fit(x_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6166565380330193\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1, max_samples=1600,\n",
      "         n_estimators=100, n_jobs=-1, oob_score=False, random_state=10,\n",
      "         verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid_bc_p.best_score_)\n",
    "print(grid_bc_p.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = 0.7185505925250684\n",
    "\n",
    "scaler = Normalizer()\n",
    "\n",
    "features = features\n",
    "\n",
    "model = BaggingClassifier(base_estimator=None, bootstrap=True,\n",
    "         bootstrap_features=False, max_features=0.75, max_samples=1000,\n",
    "         n_estimators=200, n_jobs=-1, oob_score=False, random_state=10,\n",
    "         verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=90, max_features=2, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=15,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=10, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_dict = new_unique_df[features].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(x_train_dict)\n",
    "x_train = vec.transform(x_train_dict)\n",
    "\n",
    "scaler = Normalizer()\n",
    "scaler.fit(x_train)\n",
    "x_train_sc = scaler.transform(x_train)\n",
    "\n",
    "model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=90, \n",
    "                               max_features=2, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                               min_impurity_split=None, min_samples_leaf=2, min_samples_split=15, \n",
    "                               min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1, \n",
    "                               oob_score=False, random_state=10, verbose=0, warm_start=False)\n",
    "model.fit(x_train_sc, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Return to Owner'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dog = pd.DataFrame()\n",
    "features = ['is_mixed','intake_condition', 'intake_type', 'sex', 'fixed', 'time_in_shelter', 'age_in']\n",
    "# new_dog['is_mixed'] = [1]\n",
    "# new_dog['intake_condition'] = ['Normal']\n",
    "# new_dog['intake_type'] = [\"Stray\"]\n",
    "# new_dog['sex'] = [\"Male\"]\n",
    "# new_dog['fixed'] = [\"Yes\"]\n",
    "# new_dog['time_in_shelter'] = [10000]\n",
    "# new_dog['age_in'] = [4.0]\n",
    "\n",
    "new_dog = pd.DataFrame(new_unique_df.iloc[100]).T\n",
    "\n",
    "new_dog = new_dog.to_dict(orient=\"records\")\n",
    "new_dog = vec.transform(new_dog)\n",
    "new_dog_sc = scaler.transform(new_dog)\n",
    "\n",
    "model.predict(new_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_mixed                          0\n",
       "intake_condition             Normal\n",
       "intake_type                   Stray\n",
       "sex                          Female\n",
       "fixed                            No\n",
       "time_in_shelter                3005\n",
       "age_in                      1.49315\n",
       "outcome_type        Return to Owner\n",
       "Name: A675255, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_unique_df.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_mixed</th>\n",
       "      <th>intake_condition</th>\n",
       "      <th>intake_type</th>\n",
       "      <th>sex</th>\n",
       "      <th>fixed</th>\n",
       "      <th>time_in_shelter</th>\n",
       "      <th>age_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A787254</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>93915.1</td>\n",
       "      <td>0.00821918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_mixed intake_condition intake_type     sex fixed time_in_shelter  \\\n",
       "A787254        1           Normal       Stray  Female    No         93915.1   \n",
       "\n",
       "             age_in  \n",
       "A787254  0.00821918  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unadopted_dog = unique_df.loc[\"A787254\"]\n",
    "time_in_shelter = pd.to_datetime('today') - pd.to_datetime(unadopted_dog.date_in)\n",
    "time_in_shelter = (time_in_shelter.days*24*60*60 + time_in_shelter.seconds)/60\n",
    "unadopted_dog.time_in_shelter = time_in_shelter\n",
    "\n",
    "unadopted_dog = pd.DataFrame(unadopted_dog[features]).T\n",
    "unadopted_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Adoption'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unadopted_dog_dict = unadopted_dog.to_dict(orient=\"records\")\n",
    "unadopted_dog = vec.transform(unadopted_dog_dict)\n",
    "unadopted_dog_sc = scaler.transform(unadopted_dog)\n",
    "\n",
    "model.predict(unadopted_dog_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7789646880141754"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7279596977329975"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_train==\"Adoption\",preds==\"Adoption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9632263081879792"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_train==\"Adoption\",preds==\"Adoption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Will your dog get adopted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_mixed</th>\n",
       "      <th>intake_condition</th>\n",
       "      <th>intake_type</th>\n",
       "      <th>sex</th>\n",
       "      <th>fixed</th>\n",
       "      <th>time_in_shelter</th>\n",
       "      <th>has_name</th>\n",
       "      <th>age_in</th>\n",
       "      <th>outcome_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animal_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A786884</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7132.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>Transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A706918</th>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.005479</td>\n",
       "      <td>Return to Owner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A724273</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>9994.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994521</td>\n",
       "      <td>Return to Owner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A778404</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Owner Surrender</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>4784.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.002740</td>\n",
       "      <td>Adoption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A682524</th>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4538.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.002740</td>\n",
       "      <td>Return to Owner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           is_mixed intake_condition      intake_type     sex fixed  \\\n",
       "animal_id                                                             \n",
       "A786884           1           Normal            Stray    Male   Yes   \n",
       "A706918           0           Normal            Stray  Female   Yes   \n",
       "A724273           1           Normal            Stray    Male    No   \n",
       "A778404           1           Normal  Owner Surrender    Male    No   \n",
       "A682524           1           Normal            Stray    Male   Yes   \n",
       "\n",
       "           time_in_shelter  has_name    age_in     outcome_type  \n",
       "animal_id                                                        \n",
       "A786884             7132.0         1  2.000000         Transfer  \n",
       "A706918              134.0         1  8.005479  Return to Owner  \n",
       "A724273             9994.0         1  0.994521  Return to Owner  \n",
       "A778404             4784.0         1  4.002740         Adoption  \n",
       "A682524             4538.0         1  4.002740  Return to Owner  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_unique_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_features1 = ['is_mixed','intake_condition', 'intake_type', 'sex', 'fixed', 'age_in','name', 'has_name']\n",
    "your_features2 = ['intake_condition', 'intake_type', 'sex', 'fixed', 'age_in', 'has_name']\n",
    "your_features3 = ['is_mixed', 'intake_type', 'sex', 'fixed', 'age_in', 'has_name']\n",
    "your_features4 = ['is_mixed', 'sex', 'fixed', 'age_in', 'has_name']\n",
    "your_features5 = ['intake_condition', 'sex', 'age_in', 'has_name']\n",
    "your_features6 = ['is_mixed', 'sex', 'fixed', 'age_in']\n",
    "your_features = [your_features1, your_features2, your_features3, your_features4, your_features5, your_features6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: KNeighborsClassifier \n",
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=30, n_jobs=-1)\n",
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vec), \n",
    "    (\"scaler\", scaler), \n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "\n",
    "for feat in your_features:\n",
    "    x_train_dict = new_unique_df[feat].to_dict(orient=\"records\")\n",
    "    y_train = new_unique_df['outcome_type']\n",
    "    print(feat, cross_val_score(pipeline, x_train_dict, y_train, cv=5, scoring=\"accuracy\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict = new_unique_df[features1].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), Normalizer(), MinMaxScaler(), MaxAbsScaler(), RobustScaler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   39.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:  1.7min remaining:   58.8s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:  2.0min remaining:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'scaler': [StandardScaler(copy=True, with_mean=True, with_std=True), Normalizer(copy=True, norm='l2'), MinMaxScaler(copy=True, feature_range=(0, 1)), MaxAbsScaler(copy=True), RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_kn = GridSearchCV(pipeline,param_grid=dict(scaler=scalers),cv=5, verbose=5, n_jobs=-1)\n",
    "grid_kn.fit(x_train_dict, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7065734832371113\n",
      "[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=False)), ('scaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "       with_scaling=True)), ('model', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
      "           weights='uniform'))]\n"
     ]
    }
   ],
   "source": [
    "print(grid_kn.best_score_)\n",
    "print(grid_kn.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_scaler</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.052805</td>\n",
       "      <td>2.826535</td>\n",
       "      <td>7.965209</td>\n",
       "      <td>3.085577</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>{'scaler': StandardScaler(copy=True, with_mean...</td>\n",
       "      <td>0.699190</td>\n",
       "      <td>0.706582</td>\n",
       "      <td>0.702925</td>\n",
       "      <td>0.702545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701636</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>2</td>\n",
       "      <td>0.715290</td>\n",
       "      <td>0.713788</td>\n",
       "      <td>0.716094</td>\n",
       "      <td>0.715525</td>\n",
       "      <td>0.717116</td>\n",
       "      <td>0.715563</td>\n",
       "      <td>0.001088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.966998</td>\n",
       "      <td>0.564824</td>\n",
       "      <td>5.674741</td>\n",
       "      <td>0.882539</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>{'scaler': Normalizer(copy=True, norm='l2')}</td>\n",
       "      <td>0.698810</td>\n",
       "      <td>0.707215</td>\n",
       "      <td>0.699380</td>\n",
       "      <td>0.699253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700344</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>3</td>\n",
       "      <td>0.713928</td>\n",
       "      <td>0.711731</td>\n",
       "      <td>0.715145</td>\n",
       "      <td>0.714480</td>\n",
       "      <td>0.716388</td>\n",
       "      <td>0.714334</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.812916</td>\n",
       "      <td>1.190793</td>\n",
       "      <td>4.673145</td>\n",
       "      <td>1.449552</td>\n",
       "      <td>MinMaxScaler(copy=True, feature_range=(0, 1))</td>\n",
       "      <td>{'scaler': MinMaxScaler(copy=True, feature_ran...</td>\n",
       "      <td>0.696786</td>\n",
       "      <td>0.699873</td>\n",
       "      <td>0.695327</td>\n",
       "      <td>0.698493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696647</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>4</td>\n",
       "      <td>0.711301</td>\n",
       "      <td>0.709673</td>\n",
       "      <td>0.711568</td>\n",
       "      <td>0.711537</td>\n",
       "      <td>0.712654</td>\n",
       "      <td>0.711347</td>\n",
       "      <td>0.000959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.145111</td>\n",
       "      <td>0.846617</td>\n",
       "      <td>7.430025</td>\n",
       "      <td>0.390906</td>\n",
       "      <td>MaxAbsScaler(copy=True)</td>\n",
       "      <td>{'scaler': MaxAbsScaler(copy=True)}</td>\n",
       "      <td>0.696786</td>\n",
       "      <td>0.699747</td>\n",
       "      <td>0.694821</td>\n",
       "      <td>0.698620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696495</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>5</td>\n",
       "      <td>0.711206</td>\n",
       "      <td>0.709452</td>\n",
       "      <td>0.711505</td>\n",
       "      <td>0.711537</td>\n",
       "      <td>0.712590</td>\n",
       "      <td>0.711258</td>\n",
       "      <td>0.001018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.040941</td>\n",
       "      <td>0.654370</td>\n",
       "      <td>3.469866</td>\n",
       "      <td>1.585345</td>\n",
       "      <td>RobustScaler(copy=True, quantile_range=(25.0, ...</td>\n",
       "      <td>{'scaler': RobustScaler(copy=True, quantile_ra...</td>\n",
       "      <td>0.703493</td>\n",
       "      <td>0.711013</td>\n",
       "      <td>0.706597</td>\n",
       "      <td>0.706091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706573</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718740</td>\n",
       "      <td>0.717840</td>\n",
       "      <td>0.719639</td>\n",
       "      <td>0.721190</td>\n",
       "      <td>0.721389</td>\n",
       "      <td>0.719760</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       5.052805      2.826535         7.965209        3.085577   \n",
       "1       2.966998      0.564824         5.674741        0.882539   \n",
       "2       3.812916      1.190793         4.673145        1.449552   \n",
       "3       5.145111      0.846617         7.430025        0.390906   \n",
       "4       2.040941      0.654370         3.469866        1.585345   \n",
       "\n",
       "                                        param_scaler  \\\n",
       "0  StandardScaler(copy=True, with_mean=True, with...   \n",
       "1                   Normalizer(copy=True, norm='l2')   \n",
       "2      MinMaxScaler(copy=True, feature_range=(0, 1))   \n",
       "3                            MaxAbsScaler(copy=True)   \n",
       "4  RobustScaler(copy=True, quantile_range=(25.0, ...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'scaler': StandardScaler(copy=True, with_mean...           0.699190   \n",
       "1       {'scaler': Normalizer(copy=True, norm='l2')}           0.698810   \n",
       "2  {'scaler': MinMaxScaler(copy=True, feature_ran...           0.696786   \n",
       "3                {'scaler': MaxAbsScaler(copy=True)}           0.696786   \n",
       "4  {'scaler': RobustScaler(copy=True, quantile_ra...           0.703493   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           0.706582           0.702925           0.702545  ...   \n",
       "1           0.707215           0.699380           0.699253  ...   \n",
       "2           0.699873           0.695327           0.698493  ...   \n",
       "3           0.699747           0.694821           0.698620  ...   \n",
       "4           0.711013           0.706597           0.706091  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.701636        0.003318                2            0.715290   \n",
       "1         0.700344        0.003535                3            0.713928   \n",
       "2         0.696647        0.002479                4            0.711301   \n",
       "3         0.696495        0.002604                5            0.711206   \n",
       "4         0.706573        0.002460                1            0.718740   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            0.713788            0.716094            0.715525   \n",
       "1            0.711731            0.715145            0.714480   \n",
       "2            0.709673            0.711568            0.711537   \n",
       "3            0.709452            0.711505            0.711537   \n",
       "4            0.717840            0.719639            0.721190   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.717116          0.715563         0.001088  \n",
       "1            0.716388          0.714334         0.001539  \n",
       "2            0.712654          0.711347         0.000959  \n",
       "3            0.712590          0.711258         0.001018  \n",
       "4            0.721389          0.719760         0.001374  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_kn.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K value testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict = new_unique_df[features1].to_dict(orient=\"records\")\n",
    "y_train = new_unique_df['outcome_type']\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(x_train_dict)\n",
    "x_train = vec.transform(x_train_dict)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_sc = scaler.transform(x_train)\n",
    "\n",
    "model = KNeighborsClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [5,10,20,30,40,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': [5, 10, 20, 30, 40, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_kn_k = GridSearchCV(model,param_grid=dict(n_neighbors=ks),cv=5)\n",
    "grid_kn_k.fit(x_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071558796718322\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(grid_kn_k.best_score_)\n",
    "print(grid_kn_k.best_estimator_.n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_kn_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-e88bd6737f1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_kn_k\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"param_n_neighbors\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_kn_k' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(grid_kn_k.cv_results_).set_index(\"param_n_neighbors\")['mean_test_score'].plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = 0.7071558796718322\n",
    "\n",
    "k = 40\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "features = features1\n",
    "\n",
    "model = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
    "           weights='uniform'),\n",
    "       fit_params=None, iid='warn', n_jobs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
